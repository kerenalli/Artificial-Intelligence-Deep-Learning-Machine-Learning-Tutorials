{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210217_AutoEncoder.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP40mp+CR6IzGlgjdjUYtzo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kerenalli/Artificial-Intelligence-Deep-Learning-Machine-Learning-Tutorials/blob/master/20210217_AutoEncoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9XAKA3vAF7zq",
        "outputId": "2bdc6926-e9f7-4b2e-bc85-e0c55991f0de"
      },
      "source": [
        "# train autoencoder for classification with no compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "# encoder level 1\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 2\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# bottleneck\r\n",
        "n_bottleneck = n_inputs\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder, level 1\r\n",
        "d = Dense(n_inputs)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# decoder level 2\r\n",
        "d = Dense(n_inputs*2)(d)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder_no_compress.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder_no_compress.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "42/42 - 2s - loss: 0.2423 - val_loss: 0.1747\n",
            "Epoch 2/200\n",
            "42/42 - 0s - loss: 0.0403 - val_loss: 0.0997\n",
            "Epoch 3/200\n",
            "42/42 - 0s - loss: 0.0249 - val_loss: 0.0518\n",
            "Epoch 4/200\n",
            "42/42 - 0s - loss: 0.0187 - val_loss: 0.0316\n",
            "Epoch 5/200\n",
            "42/42 - 0s - loss: 0.0159 - val_loss: 0.0192\n",
            "Epoch 6/200\n",
            "42/42 - 0s - loss: 0.0135 - val_loss: 0.0143\n",
            "Epoch 7/200\n",
            "42/42 - 0s - loss: 0.0123 - val_loss: 0.0121\n",
            "Epoch 8/200\n",
            "42/42 - 0s - loss: 0.0131 - val_loss: 0.0095\n",
            "Epoch 9/200\n",
            "42/42 - 0s - loss: 0.0111 - val_loss: 0.0077\n",
            "Epoch 10/200\n",
            "42/42 - 0s - loss: 0.0101 - val_loss: 0.0078\n",
            "Epoch 11/200\n",
            "42/42 - 0s - loss: 0.0103 - val_loss: 0.0054\n",
            "Epoch 12/200\n",
            "42/42 - 0s - loss: 0.0092 - val_loss: 0.0054\n",
            "Epoch 13/200\n",
            "42/42 - 0s - loss: 0.0103 - val_loss: 0.0056\n",
            "Epoch 14/200\n",
            "42/42 - 0s - loss: 0.0085 - val_loss: 0.0050\n",
            "Epoch 15/200\n",
            "42/42 - 0s - loss: 0.0089 - val_loss: 0.0064\n",
            "Epoch 16/200\n",
            "42/42 - 0s - loss: 0.0085 - val_loss: 0.0059\n",
            "Epoch 17/200\n",
            "42/42 - 0s - loss: 0.0073 - val_loss: 0.0043\n",
            "Epoch 18/200\n",
            "42/42 - 0s - loss: 0.0077 - val_loss: 0.0038\n",
            "Epoch 19/200\n",
            "42/42 - 0s - loss: 0.0075 - val_loss: 0.0032\n",
            "Epoch 20/200\n",
            "42/42 - 0s - loss: 0.0067 - val_loss: 0.0037\n",
            "Epoch 21/200\n",
            "42/42 - 0s - loss: 0.0075 - val_loss: 0.0039\n",
            "Epoch 22/200\n",
            "42/42 - 0s - loss: 0.0073 - val_loss: 0.0032\n",
            "Epoch 23/200\n",
            "42/42 - 0s - loss: 0.0074 - val_loss: 0.0036\n",
            "Epoch 24/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0041\n",
            "Epoch 25/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0028\n",
            "Epoch 26/200\n",
            "42/42 - 0s - loss: 0.0067 - val_loss: 0.0031\n",
            "Epoch 27/200\n",
            "42/42 - 0s - loss: 0.0066 - val_loss: 0.0033\n",
            "Epoch 28/200\n",
            "42/42 - 0s - loss: 0.0065 - val_loss: 0.0038\n",
            "Epoch 29/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0030\n",
            "Epoch 30/200\n",
            "42/42 - 0s - loss: 0.0060 - val_loss: 0.0029\n",
            "Epoch 31/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0024\n",
            "Epoch 32/200\n",
            "42/42 - 0s - loss: 0.0057 - val_loss: 0.0024\n",
            "Epoch 33/200\n",
            "42/42 - 0s - loss: 0.0059 - val_loss: 0.0027\n",
            "Epoch 34/200\n",
            "42/42 - 0s - loss: 0.0063 - val_loss: 0.0022\n",
            "Epoch 35/200\n",
            "42/42 - 0s - loss: 0.0056 - val_loss: 0.0024\n",
            "Epoch 36/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0030\n",
            "Epoch 37/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
            "Epoch 38/200\n",
            "42/42 - 0s - loss: 0.0058 - val_loss: 0.0025\n",
            "Epoch 39/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0025\n",
            "Epoch 40/200\n",
            "42/42 - 0s - loss: 0.0056 - val_loss: 0.0036\n",
            "Epoch 41/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0029\n",
            "Epoch 42/200\n",
            "42/42 - 0s - loss: 0.0054 - val_loss: 0.0031\n",
            "Epoch 43/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0021\n",
            "Epoch 44/200\n",
            "42/42 - 0s - loss: 0.0056 - val_loss: 0.0041\n",
            "Epoch 45/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0032\n",
            "Epoch 46/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0030\n",
            "Epoch 47/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0026\n",
            "Epoch 48/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0026\n",
            "Epoch 49/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0023\n",
            "Epoch 50/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0025\n",
            "Epoch 51/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0019\n",
            "Epoch 52/200\n",
            "42/42 - 0s - loss: 0.0049 - val_loss: 0.0027\n",
            "Epoch 53/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0026\n",
            "Epoch 54/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0027\n",
            "Epoch 55/200\n",
            "42/42 - 0s - loss: 0.0054 - val_loss: 0.0019\n",
            "Epoch 56/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0030\n",
            "Epoch 57/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
            "Epoch 58/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0023\n",
            "Epoch 59/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0025\n",
            "Epoch 60/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0022\n",
            "Epoch 61/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0021\n",
            "Epoch 62/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0018\n",
            "Epoch 63/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0016\n",
            "Epoch 64/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0021\n",
            "Epoch 65/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 66/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0020\n",
            "Epoch 67/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0016\n",
            "Epoch 68/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0034\n",
            "Epoch 69/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 70/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0026\n",
            "Epoch 71/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0018\n",
            "Epoch 72/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0019\n",
            "Epoch 73/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0022\n",
            "Epoch 74/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 75/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0025\n",
            "Epoch 76/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0018\n",
            "Epoch 77/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0038\n",
            "Epoch 78/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0022\n",
            "Epoch 79/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0029\n",
            "Epoch 80/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0024\n",
            "Epoch 81/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 82/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0025\n",
            "Epoch 83/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0020\n",
            "Epoch 84/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 85/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0022\n",
            "Epoch 86/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0028\n",
            "Epoch 87/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0012\n",
            "Epoch 88/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0021\n",
            "Epoch 89/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0022\n",
            "Epoch 90/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0022\n",
            "Epoch 91/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0020\n",
            "Epoch 92/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0033\n",
            "Epoch 93/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0022\n",
            "Epoch 94/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0017\n",
            "Epoch 95/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0015\n",
            "Epoch 96/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0019\n",
            "Epoch 97/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0012\n",
            "Epoch 98/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 99/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 100/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0018\n",
            "Epoch 101/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0013\n",
            "Epoch 102/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 103/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0020\n",
            "Epoch 104/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 105/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0018\n",
            "Epoch 106/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0034\n",
            "Epoch 107/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0022\n",
            "Epoch 108/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0010\n",
            "Epoch 109/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 110/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0015\n",
            "Epoch 111/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0018\n",
            "Epoch 112/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0023\n",
            "Epoch 113/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0016\n",
            "Epoch 114/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 115/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 116/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 117/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0016\n",
            "Epoch 118/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0023\n",
            "Epoch 119/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0026\n",
            "Epoch 120/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0015\n",
            "Epoch 121/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 122/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 123/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0015\n",
            "Epoch 124/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0020\n",
            "Epoch 125/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 126/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 127/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0021\n",
            "Epoch 128/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0022\n",
            "Epoch 129/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0017\n",
            "Epoch 130/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0026\n",
            "Epoch 131/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0013\n",
            "Epoch 132/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0018\n",
            "Epoch 133/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0016\n",
            "Epoch 134/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0017\n",
            "Epoch 135/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0013\n",
            "Epoch 136/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 137/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 138/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0018\n",
            "Epoch 139/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 140/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0012\n",
            "Epoch 141/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 142/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0020\n",
            "Epoch 143/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0020\n",
            "Epoch 144/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 145/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 146/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 147/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 148/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
            "Epoch 149/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0011\n",
            "Epoch 150/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 151/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0029\n",
            "Epoch 152/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0013\n",
            "Epoch 153/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0010\n",
            "Epoch 154/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 155/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0013\n",
            "Epoch 156/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 157/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0015\n",
            "Epoch 158/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 159/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
            "Epoch 160/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 161/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
            "Epoch 162/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0015\n",
            "Epoch 163/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0013\n",
            "Epoch 164/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0012\n",
            "Epoch 165/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 166/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0019\n",
            "Epoch 167/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0011\n",
            "Epoch 168/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 169/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 170/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 171/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
            "Epoch 172/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 173/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0017\n",
            "Epoch 174/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 9.5395e-04\n",
            "Epoch 175/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0013\n",
            "Epoch 176/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0011\n",
            "Epoch 177/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 178/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0011\n",
            "Epoch 179/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
            "Epoch 180/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0011\n",
            "Epoch 181/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0012\n",
            "Epoch 182/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 8.6969e-04\n",
            "Epoch 183/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0011\n",
            "Epoch 184/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0016\n",
            "Epoch 185/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0012\n",
            "Epoch 186/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0013\n",
            "Epoch 187/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 9.1374e-04\n",
            "Epoch 188/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 8.7502e-04\n",
            "Epoch 189/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 8.8737e-04\n",
            "Epoch 190/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0011\n",
            "Epoch 191/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0014\n",
            "Epoch 192/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0011\n",
            "Epoch 193/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0020\n",
            "Epoch 194/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 9.2027e-04\n",
            "Epoch 195/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0010\n",
            "Epoch 196/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0010\n",
            "Epoch 197/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0012\n",
            "Epoch 198/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0010\n",
            "Epoch 199/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0010\n",
            "Epoch 200/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 9.8224e-04\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXRcd3338fd3Vu2r5X2RnTgh+yaSQAhLIcSBkrAVAk2btvS49Cl94PRACQcaSvqcU2ifh7achxJC8dOFQkqhKe6D82SBhBaSgJ3djp14iRfJji3L2pdZv88f90oeyZItx5JGXH9e58zRzF1mvnNn9Lm/+7t37jV3R0REoitW7gJERGR2KehFRCJOQS8iEnEKehGRiFPQi4hEnIJeRCTiEtOZyMzWAX8DxIG/c/cvThj/R8DvAnmgE/gdd98XjisAz4eT7nf3m0/2WgsWLPDW1tbTeQ8iIme9J5988qi7t0w27pRBb2Zx4KvADUA7sNnMNrr7CyWTPQ20ufuQmf0+8BfAB8Nxw+5++XSLbW1tZcuWLdOdXEREADPbN9W46XTdXA3scvc97p4F7gVuKZ3A3R9x96Hw4RPA8ldbrIiIzKzpBP0y4EDJ4/Zw2FQ+Atxf8rjCzLaY2RNm9u5XUaOIiJyBafXRT5eZ3Qa0AW8qGbzK3TvMbA3wYzN73t13T5hvPbAeYOXKlTNZkojIWW86Qd8BrCh5vDwcNo6ZvQ34LPAmd8+MDnf3jvDvHjN7FLgCGBf07n4PcA9AW1ubTr4jIqctl8vR3t7OyMhIuUuZVRUVFSxfvpxkMjnteaYT9JuBtWa2miDgbwU+XDqBmV0BfB1Y5+5HSoY3AkPunjGzBcB1BDtqRURmVHt7O7W1tbS2tmJm5S5nVrg7XV1dtLe3s3r16mnPd8o+enfPAx8DHgC2A991921mdpeZjR4q+ZdADfCvZvaMmW0Mh18AbDGzZ4FHgC9OOFpHRGRGjIyM0NzcHNmQBzAzmpubT3urZVp99O6+Cdg0YdidJfffNsV8jwGXnFZFIiKvUpRDftSreY+R+WXsYCbPlx98kWcO9JS7FBGReSUyQT+SK/CVH+/iuXYFvYjMvZ6eHv72b//2tOd7xzveQU/P7OZWZII+Fm7OFIs6aEdE5t5UQZ/P508636ZNm2hoaJitsoAZPo6+nEaDvqCcF5EyuOOOO9i9ezeXX345yWSSiooKGhsb2bFjBy+99BLvfve7OXDgACMjI3z84x9n/fr1wPHTvgwMDHDTTTfxhje8gccee4xly5bxgx/8gMrKyjOuLTpBH26b6Bq4IvKF/9jGCwf7ZvQ5L1xax+ffddGU47/4xS+ydetWnnnmGR599FHe+c53snXr1rHDIDds2EBTUxPDw8O89rWv5X3vex/Nzc3jnmPnzp185zvf4Rvf+AYf+MAH+P73v89tt912xrVHJ+hHW/TquhGReeDqq68ed6z7V77yFe677z4ADhw4wM6dO08I+tWrV3P55cE5IK+66ir27t07I7VEJujjsbCPXjkvctY7Wct7rlRXV4/df/TRR3n44Yd5/PHHqaqq4s1vfvOkx8Kn0+mx+/F4nOHh4RmpJTI7Y0cPLS2q60ZEyqC2tpb+/v5Jx/X29tLY2EhVVRU7duzgiSeemNPaItOi11E3IlJOzc3NXHfddVx88cVUVlayaNGisXHr1q3j7rvv5oILLuD888/n2muvndPaIhP0cVPXjYiU17e//e1Jh6fTae6///5Jx432wy9YsICtW7eODf/kJz85Y3Wp60ZEJOIiFPSGmYJeRGSiyAQ9BN03CnoRkfEiFfQxM/XRi4hMEK2gj+moGxGRiaIV9Oq6ERE5QeSCvlAsdxUicjZ6tacpBvjrv/5rhoaGZrii4yIW9DrqRkTKYz4HfWR+MAUQi5nOXikiZVF6muIbbriBhQsX8t3vfpdMJsN73vMevvCFLzA4OMgHPvAB2tvbKRQK/Mmf/AmHDx/m4MGDvOUtb2HBggU88sgjM15btILejIKCXkTuvwNeeX5mn3PxJXDTF6ccXXqa4gcffJDvfe97/OIXv8Ddufnmm/nP//xPOjs7Wbp0KT/84Q+B4Bw49fX1fPnLX+aRRx5hwYIFM1tzKGJdNzq8UkTK78EHH+TBBx/kiiuu4Morr2THjh3s3LmTSy65hIceeohPf/rT/Nd//Rf19fVzUk/EWvS68IiIcNKW91xwdz7zmc/we7/3eyeMe+qpp9i0aROf+9zneOtb38qdd9456/VErkWvC4+ISDmUnqb4xhtvZMOGDQwMDADQ0dHBkSNHOHjwIFVVVdx222186lOf4qmnnjph3tkQqRZ9PKauGxEpj9LTFN900018+MMf5nWvex0ANTU1fOtb32LXrl186lOfIhaLkUwm+drXvgbA+vXrWbduHUuXLp2VnbE237o62trafMuWLa9q3jd86cdc3drElz94+QxXJSLz3fbt27ngggvKXcacmOy9mtmT7t422fSR6roJWvTza8UlIlJukQp6HXUjInKiSAW9GTqOXuQsNt+6omfDq3mPkQr6uOmXsSJnq4qKCrq6uiKdAe5OV1cXFRUVpzVfpI66iZlR1EnNRM5Ky5cvp729nc7OznKXMqsqKipYvnz5ac0TqaBX143I2SuZTLJ69epylzEvRavrRic1ExE5wbSC3szWmdmLZrbLzO6YZPwfmdkLZvacmf3IzFaVjLvdzHaGt9tnsviJdNSNiMiJThn0ZhYHvgrcBFwIfMjMLpww2dNAm7tfCnwP+Itw3ibg88A1wNXA582scebKHy8W0ykQREQmmk6L/mpgl7vvcfcscC9wS+kE7v6Iu4+eNf8JYHRPwY3AQ+5+zN27gYeAdTNT+ol04RERkRNNJ+iXAQdKHreHw6byEeD+VznvGdE1Y0VETjSjR92Y2W1AG/Cm05xvPbAeYOXKla/69eM6vFJE5ATTadF3ACtKHi8Ph41jZm8DPgvc7O6Z05nX3e9x9zZ3b2tpaZlu7Scwdd2IiJxgOkG/GVhrZqvNLAXcCmwsncDMrgC+ThDyR0pGPQC83cwaw52wbw+HzQp13YiInOiUXTfunjezjxEEdBzY4O7bzOwuYIu7bwT+EqgB/tXMAPa7+83ufszM/oxgZQFwl7sfm5V3QnAcfbYwW88uIvLLaVp99O6+Cdg0YdidJfffdpJ5NwAbXm2Bp0NdNyIiJ4rcL2OLOo5eRGScSAW9fhkrInKiiAW9um5ERCaKWNDrFAgiIhNFLujVoBcRGS9aQR/T+ehFRCaKVtDrB1MiIieIXNAr50VExotU0Md1PnoRkRNEKuj1y1gRkRNFKujVdSMicqJIBX1cx9GLiJwgUkEfi6nrRkRkokgFvelcNyIiJ5jRSwmWVSHHkpGXqSvmyl2JiMi8Ep0W/XA3f/jib/A2f6zclYiIzCvRCfp4KvjjatGLiJSKXNAnFfQiIuNEJ+gT6eCP58tciIjI/BKdoI/FKRIjiVr0IiKlohP0QMGSatGLiEwQraCPJUmoRS8iMk6kgr5oSXXdiIhMEKmgL8SSJL2A6zQIIiJjIhf0KcvpNAgiIiUiFfRB101eJzYTESkRraCPpUgp6EVExolY0CeDoC+WuxIRkfkjckGvrhsRkfEiFfSFWIqkKehFREpFKug9liRFTl03IiIlIhX0QR99QS16EZES0wp6M1tnZi+a2S4zu2OS8W80s6fMLG9m758wrmBmz4S3jTNV+GSCo25yCnoRkRKnvJSgmcWBrwI3AO3AZjPb6O4vlEy2H/gt4JOTPMWwu18+A7Wekoc7YwsKehGRMdO5ZuzVwC533wNgZvcCtwBjQe/ue8NxZe0dL8ZTpC2Pcl5E5LjpdN0sAw6UPG4Ph01XhZltMbMnzOzdp1XdafLwOPqCzoEgIjJmOi36M7XK3TvMbA3wYzN73t13l05gZuuB9QArV6581S9UjAe/jB1Rk15EZMx0WvQdwIqSx8vDYdPi7h3h3z3Ao8AVk0xzj7u3uXtbS0vLdJ/6xNcKD69UzouIHDedoN8MrDWz1WaWAm4FpnX0jJk1mlk6vL8AuI6Svv2Z5vFwZ6y6bkRExpwy6N09D3wMeADYDnzX3beZ2V1mdjOAmb3WzNqBXwO+bmbbwtkvALaY2bPAI8AXJxytM6M8liZuTrGgywmKiIyaVh+9u28CNk0YdmfJ/c0EXToT53sMuOQMa5w2jyeDv/nsXL2kiMi8F6lfxhJLAeCFTJkLERGZPyIV9B4fDXq16EVERkUz6HMKehGRUZEKesKgR103IiJjIhX0x3fGKuhFREZFKuhHW/Q66kZE5LiIBX0aAC/kylyIiMj8EbGgD7puTH30IiJjIhX0pq4bEZETRCroPey6QV03IiJjIhX0lgha9Oq6ERE5LlJBT0K/jBURmShaQT/2gykFvYjIqEgF/VjXTV599CIio6IV9KMt+qJa9CIioyIV9IztjFXQi4iMilTQW6IiuKOgFxEZE7GgH/1lrIJeRGRUpII+FktQdMP0gykRkTERC/oYWRLaGSsiUiJaQW+QJUFMXTciImMiFfTxmJEjgalFLyIyJlJBHzMjSxIr5stdiojIvBGpoDeDnMd1UjMRkRKRCvp4LGjRx4o66kZEZFSkgj5mYR+9dsaKiIyJVNDb6FE3atGLiIyJVNDHzRT0IiITRCroY2bkPEFMh1eKiIyJVtCHx9GrRS8icly0gt4Ij7pRi15EZFTEgt7IEleLXkSkxLSC3szWmdmLZrbLzO6YZPwbzewpM8ub2fsnjLvdzHaGt9tnqvDJjB1H7wp6EZFRpwx6M4sDXwVuAi4EPmRmF06YbD/wW8C3J8zbBHweuAa4Gvi8mTWeedlT1Uq4M1ZBLyIyajot+quBXe6+x92zwL3ALaUTuPted38OKE6Y90bgIXc/5u7dwEPAuhmoe1KjP5iKK+hFRMZMJ+iXAQdKHreHw6bjTOY9bTqOXkTkRPNiZ6yZrTezLWa2pbOz8wyeBzKkSBR1UjMRkVHTCfoOYEXJ4+XhsOmY1rzufo+7t7l7W0tLyzSf+kRmxggpEp6F4sReJBGRs9N0gn4zsNbMVptZCrgV2DjN538AeLuZNYY7Yd8eDps1I6SDO/nh2XwZEZFfGqcMenfPAx8jCOjtwHfdfZuZ3WVmNwOY2WvNrB34NeDrZrYtnPcY8GcEK4vNwF3hsFmTsTDocwp6ERGAxHQmcvdNwKYJw+4sub+ZoFtmsnk3ABvOoMbTcjzoh+bqJUVE5rV5sTN2Jo113ahFLyICRDDo1aIXERkvckGfVR+9iMg4kQt6tehFRMaLXNBnrSK4oxa9iAgQwaDX4ZUiIuNFLuiz6roRERknckGfi6nrRkSkVPSCXi16EZFxIhj0SYqYWvQiIqHIBX08HiNnFQp6EZFQ5II+ZkY2llbXjYhIKHJBbxYeeaMWvYgIEMGgj5uRjVWoRS8iEopc0MfM1KIXESkRvaCPWfDrWAW9iAgQxaAf66NX142ICEQy6I2MDq8UERkTvaAf67pRi15EBKIY9AYZUmrRi4iEIhj06roRESkVuaCPm4UtenXdiIhABIPeDEZIQzEPhVy5yxERKbvIBX08ZozoVMUiImMiF/Qxs6BFD+qnFxEhgkE/1nUDatGLiBDBoB/fdaMWvYhI5II+Zsawp4IHCnoRkSgGPYwwGvTquhERiWDQG8PaGSsiMiaaQe/aGSsiMipyQR+PGcOoj15EZNS0gt7M1pnZi2a2y8zumGR82sz+JRz/czNrDYe3mtmwmT0T3u6e2fInq5WSnbFq0YuIJE41gZnFga8CNwDtwGYz2+juL5RM9hGg293PNbNbgS8BHwzH7Xb3y2e47impj15EZLzptOivBna5+x53zwL3ArdMmOYW4B/C+98D3mpmNnNlTl88Zgx5CjAY6StHCSIi88p0gn4ZcKDkcXs4bNJp3D0P9ALN4bjVZva0mf3EzK4/w3pPyQxyHoOqZhg8MtsvJyIy752y6+YMHQJWunuXmV0F/LuZXeTu45raZrYeWA+wcuXKM3rBmBnuQO0iGOg8o+cSEYmC6bToO4AVJY+Xh8MmncbMEkA90OXuGXfvAnD3J4HdwHkTX8Dd73H3Nndva2lpOf13USJuRtEdahbCwOEzei4RkSiYTtBvBtaa2WozSwG3AhsnTLMRuD28/37gx+7uZtYS7szFzNYAa4E9M1P65GIxKBRHg15dNyIip+y6cfe8mX0MeACIAxvcfZuZ3QVscfeNwDeBfzKzXcAxgpUBwBuBu8wsBxSBj7r7sdl4I6NiZhSdIOgHj4B70HEvInKWmlYfvbtvAjZNGHZnyf0R4Ncmme/7wPfPsMbTEvTRO9QsgvwIZPqgon4uSxARmVci98vYmEHBHaoXBgPUfSMiZ7noBX3MKI720YOCXkTOepEL+rgZuULYdQM68kZEznqRC/qFdWmGcwX6k43BgEEdSy8iZ7fIBf2yhioA2kcqweJq0YvIWS96Qd9YCUBHT0Y/mhIRIYpB3zAa9MNQ3aLTIIjIWS9yQb+gJkU6EQuCvmaRWvQictaLXNCbGcsaKmnvHgqCXjtjReQsF7mgh6CfvqN7+HgffbFY7pJERMommkHfUBl03TSshGIe+trLXZKISNlEMuiXN1ZydCBLpuGcYMDRneUtSESkjCIZ9KOHWB5KLA8GdO0qYzUiIuUVzaAPfzS1P1MDqVoFvYic1aIZ9KM/muodgQXnqutGRM5qkQz6xXUVVKfiPNfeC81r1aIXkbNaJIM+HjOuX9vCj3ccxpvPgd4DkB0qd1kiImURyaAHeOsFCzncl+FAbFkw4NisXqpWRGTeimzQ/8prFmIG/3WsIRjQpX56ETk7RTbom2vSXLmykfv2Bztm6XyxvAWJiJRJZIMe4KaLF7PlYIbepkth+3+Ae7lLEhGZc5EO+t943SrOX1TL3b3XwOGt8Mpz5S5JRGTORTro04k4//PXLuPe4avJWRJ/+lvlLklEZM5FOugBLllez0dvvIr/l7+KzFP/AkPHyl2SiMicinzQA6x/4xq2td6O5QbZ89X38qUfPsdtf/fz4Jz1IiIRd1YEvZnxid/8IPev+SxrBp9m2RNf4Ml93Xz4Gz/nld6RcpcnIjKrzOfZkShtbW2+ZcuWWXv+vh98mrqn7+blX/ka7/pRMzGD37puNe7O2kW1/OolS3jwhcOkEzHe8pqFs1aHiMhMMrMn3b1t0nFnW9CTz8KGG6FrN/tv+Vf+5AnjJy8dv9xgS22azv4MAJ975wWsaq4mky9w2fIGVjRVzV5dIiJnQEE/Ufc++D83QW4Y3vAJBryC1Gtv577njvBvT3Vw8+VLeWTHER7efmTcbDddvJjbX9/KgWNDVKbinLuwhtcsrqN3OMf2Q300VqWIx6AqlWBpQ+XsvgcRkRIK+skc2wP/cAv07g8eX/NRuOlLY6NzhSIPv3CY5X6Q2sF9bOpt5a9/ephsfvz1Zy9eVseezkGGsoVxw9912VLecn4LuUKR3Z2DZHIFKlMJqlJxqlJx6iqSXLK8nmQ8xu7OAdpWNdJck6ZYdF460k8u71y8rA4zm/VFISK//BT0U8lnITcIj34Jfv41uOq3IZ6C/oNQ2QRNa+DRP4f8CMSSdF/3ObYs+RDntFSTKzg/23WU+57uYO2iGt516VIGs3ncYccrfWz46V6Gc0H4pxIxqlJxhjIFsoXJL1SejBsrm6o40pehP5MH4NyFNbQ2VxEzI2bGob4RsvkiN1y4iEyuwJ6jg9SkE9RXJqmrTFJfmeToQIZDPcOcu7CGRDzGoZ5hGqpSLKqroLkmRd9wjnzRqatIUleZoK4iSXU6Qf9Ijs7+DEf6M3T2ZxjM5KlKJ8jkCmTyRV6zuJa21ibOaak+5cqnWHS2HuxlKFvgypWNpBIxMvkCz7X3sqAmzaqmKmKx48+RyRdIxGLEY1M/b7Ho5MKLvKcT8dP6mPtGcuzpHOTchTXUpBOnNa/ILwsF/anks3Dvh2HPo5CsgtpF0HcQsgPQej1c9wnY8k14cRNc9iG45P2w6jpIVARbBr3tEEvAymvBYnB0J8P7NnOs7iKKC85jaUNlEGLu5Dp3MmxVHLNGnj7QTa7grGyq4kfbD3Pg2DAL69JctryBXKHIxmcP0juco1B0iu4srK0gWyiyee8xEjHjnOYKBnLQO5yjfyRYOTTHBvn1ysf5zuBVdNJIbTrBQLgCOh2VyTjDuQLpRBDAo1ssTdUp3J1kPEZTdYqGqiQ9Qzn2HB0kbkZFMkbRg5oAqlJxmmtSdA1kx54jGTdqK5Kc21JDdTrOz3Z1EY8ZK5oqyRed6lSCylScoc59HB6Oc7RYNVa/GVy8tJ6Ll9WRiMXYfqiPRNx45yVL+Omuoxw4NszlK4MT2Q2M5Cm688iOIwxmC5jB6uZqLlxax0VL6xnK5nn56CAjuSLZQpGYwbVrmlmzoJpsoUhlMo4ZDGYKDGXzdPZn6OgZJhWPsaShkstXNFCTTtA9lOXFV/p56XA/RweyNFenaF1QTWtzNUV3coVieHPSiRi1FQl6h3MUHWrSCWoqEtSmE8RixrMHesgVily7ppl80dlxqJ/n2nu4YEkd165pZiCT44VD/XQNZLhiZSOL6yqC95rJcbBnhMN9I8RjRuuCaq5a1chgJs/hvgzdg1kqU3EaqoIGQX1lkmODWbZ29NJUnaalNk2+ECyHimSc1c3V7DwywJ7OAV6zpA53p38kz7LGSpqrU7jDM+099A7laKhK0lCVIm5GJl+gqTpFRTJOz3COZw/00DucY0VjFauaq1hSX0EiHsPdKRSdRHzqA/8KRaeje5jBbJ7FdRU0VqfGjR/OFnCcqpRW3jADQW9m64C/AeLA37n7FyeMTwP/CFwFdAEfdPe94bjPAB8BCsB/d/cHTvZaZQn6yeSG4ch2WHwpxBNQLMDDn4ef3wOFDMTTUNkAA4ePz1OzCApZGO4OHsdTcM3vQSEHR1+Cw9uC6WMJuOBd0HxusNXQ+gaoqIf+V4KTry2+BBpbg8e7fwzde6FhZTBt7WJ6e7up+dmfE3/5UThvHdSvoFjMM3jBB6n+0R3EOrbgiUrya95KsnEF+UWX0JNcRN9wnvSCFcSrGhga6ONYvIX+4RHS7Y9TnTTqaqppSOapWXMNyXQVxc0bMIr4wotpT69h874e9rS/wmDFEoaKMbqHcvQMZalJJzh3YQ0AI7ki+aLTtqqRmooET+zq5Pp9/5s12R10XvlxOr2RA7059rKE7Yf66BnK8ubzgzONdnQPU2tD9GZjNA7u4c96Pk0uXs3/Pe9/0Jh7haQV2VXTxsMdCfZ2DZHK9rJk0SKODubY2zVEY1WStkWQPPgkLyQugIp6cgXnmjVNvPU1i9jdOcDWjl6OduziI0PfpMNb+EHdhyimG0glYgxn87x0eOCkX4sFNSmy+SJ94Yr1OOeOqo28zbbwV/w6Pxx8zRl9/WIUabVXyJKkJ7WEgczE15ueBvrJkWCQ09tnlErETuimHFWTTpBOxOgazJ7yedpsB9WW4SfFy4DgWhGL6yroHsqSLzgXLasjX3C6BjLEYkZjVdB46B/Js/NwP4Nh42B0JZ0tFEnEjMpUgp2H+wG4aGkd6USc7qEsh3pHMIOGqiRL6iupTMbJF4v0DOWIx4z6yiRrF9aSyRfY1zXE/mNDLKpLc+GSOvYcHSRfcJpqUhQKzsHeYQ72DNO2qomFdWleOtxP73CeqlSctQtrODqQoWsw+P4va6ikpTZNJl9kJFdgJNwSrkzGg0ZLJmzkJIxUPE6uUGQwk2cgk6c6nWBFUxXpRIwl9RW898rlp/VZjTqjoDezOPAScAPQDmwGPuTuL5RM89+AS939o2Z2K/Aed/+gmV0IfAe4GlgKPAyc5+6Fia8zat4E/VSyQ7DvMdj9Ixg4Aq3XBVexGuyEF34A6VpYcQ0sugh++lewfSOkamDBWlhwftDqP/oSPPddGOoCplj+iYqgy2gqqRq48BbY+VCwUirmIT8cbFG84y+h/Uno2AK9HUH31GSqmsHiMDh+pzPxVLDiGeycfD6LQ91SqF0c3pZCRR10PAm5EWheAwOdwXsrFoJlVdEAIz3Hn2PVdVDdAv2HghVa/YrgNXc+EGxVmUG6DrwIfR2lLw5tvx0Mf/LvoaIeX3EtXbXn03jsWeL7fgpeCFai130CnvnnYP7cMOQzwecz3IN7EfIjWEUdnHsDLLkMUlUMHniWoVgtw61vo9h3iFhuiGR1PTXHtlERK5Jc+xawGIN7n2Rk50841nQ52SVtnLvvXtI7fwiVjTDcTfbiD7H/0j+kumsryfwAcYN0907iHZuJd+8i2/oW8gsuJD/cR2/tufRULCebL7I21kHl/p+Q3PUAicIQbjFo+wgdzdeydyBBuqqe1toCdX07GdjxIwYSzRxrvoKqWI5FfVup7XoORvoYSC+k05pZffQRiskaDrV9kuJQL8XBowxYNbmRYUhX07D6CnK9hxkZ7CNfvYi6of0Uh7p5mtdQs/gczl1Yx9EDL7GoewvN/dspjgxw1JrYn1zD8kXNVDcvpzveTPzI86SGj2KJFC/XXEm26Fz6yn20vhK06/Zf+nF+seTDdB8+wKKOhyhWt9BTsYL2w500xoZZmMqQLAyzr9jCy4UFXOIvsapiiIX11XjNIl7O1PNMbwWWrqNQLJAYPMyNsc24xfhe/nryxFiQzLKgthInxrEM7BuM4/ksS4uHuIyd7Euew6OFS+k9sp9UMklTUzMX1QxwoC/PE50p1i6soiZepH9omHSsSEVtE431DTy26whLM3v4lfpDZKqXsj/fyP6uQXK1K2ioq6V/JEd79zBdg1nqEnnOSRylKTFCb6KF12cfY01hD99P/io7Y+eQKxTJ5gtUx3LUpyGWrqUvU6SjZxiAy1c08O9/cN3U//cncaZB/zrgT939xvDxZwDc/c9LpnkgnOZxM0sArwAtwB2l05ZON9XrzfugP13DPUGATdavXSzCkW2w/4lgS6CyMVhpdGwJuoPql8Oq18PCC4OrZB17OVi5mMHqNwZh6x48HjwKj38VFl8MF7+v5DUKwYplIAzznn2QHYREGvb/HDvpaP0AAAjMSURBVHJDQXdU9YJgxWJx2HZfMN31nwxWUIe3BTczSFUHWxi97UFI9x0Ku7n6gzrTdXBsN9QsDgK3ex9c/0fwuj+Abf8O8WQQvE/9E+BQuwRqFkLX7mBr56L3wEgfHHkB3vuN4PW2fg9WXAupqmC+X3w9eC9X/TYUc7Dv8eB6AwvOC7aUWi6A+/8Yho8FW0ErrglWnIkKGOkNVoZv+uNgxfP43wZbTaMru3RdsHwmtkUsHsxXzB0f1rgaul8O7sfT8OY74Nrfh5/8Bfzsb058jngallwazLfzgeO1+ISWc2VT8D5WXhusPLdsOHEagIZVwec+uiKvqA+WU1VTcPnMozuD78KhZ4PvFECi8niDYLLnBIglx7/P0WFLLgtW6N37gs94ong6aHCMvu9kFbz+v0PPfnj225O/1pmIJQkaE6extRNLnMb0BnVL8cFOrDDJ1ovFoW4ZxGJgMTw7hA28cuJ0icrgf6u6Jfh/yw0dX/YWg2QVbgYWo7jkcuK3b5z++ykt5wyD/v3AOnf/3fDxbwDXuPvHSqbZGk7THj7eDVwD/CnwhLt/Kxz+TeB+d//eVK8XuaA/W+SzkEidOHx0RTSTjmwP/lkXX3J8WHYoWBGM6t4XnK30vJuCrreTcYdMP2T6gq2T4WOw72dBkFbUwVA3tJwXTHvgF8EKo7EV6pfBkR1B6K1+Y7C1MOrQc7DrIVj5umCF7cVgqyUW7kgu5IKVezwdrNT6DwV1NK2B5nOOTwfBFk/fwbDG/uB1GluhcVWwldK9N9y3tGTy91osQPtmaDoHalqCzyqeDLoYj2wPtsrStcEKuGFV8FwdTwYrv2IhCLPFl0C65vhz5rNBF2ZvRzDfoouDfVvZIdjzSPB+z/mVYEXtDjsfhFeeD7YWL35v8D5624MVa2VD8PqJiqDrsmc/LG8LuiuL+bBBcTBYDtnB4PtUUR/sP8tn4MUfQrI6+Kzcg9fOZyDTGzxnzWJYdmVQV8dTwfJ1D/bB1S0LQnjgSLDMY8lg2cQSQcPj2J6gS3bRRbDsqqDmwc5g/qMvBu+f8DXj6fBzaQ3eT+8BWHpF0Fh64u7gII9kdfA9TVYFy2K4O9jaHH2O+uXw+j88xT/A5OZ90JvZemA9wMqVK6/at2/fq3mfIiJnrZMF/XTOddMBrCh5vDwcNuk0YddNPcFO2enMi7vf4+5t7t7W0tIyjZJERGS6phP0m4G1ZrbazFLArcDETqSNwO3h/fcDP/ZgU2EjcKuZpc1sNbAW+MXMlC4iItNxygNQ3T1vZh8DHiA4vHKDu28zs7uALe6+Efgm8E9mtgs4RrAyIJzuu8ALQB74g5MdcSMiIjNPP5gSEYmAM+2jFxGRX2IKehGRiFPQi4hEnIJeRCTi5t3OWDPrBM7kF1MLgKMzVM5MUl2nZ77WBfO3NtV1euZrXfDqalvl7pP+EGneBf2ZMrMtU+15LifVdXrma10wf2tTXadnvtYFM1+bum5ERCJOQS8iEnFRDPp7yl3AFFTX6ZmvdcH8rU11nZ75WhfMcG2R66MXEZHxotiiFxGREpEJejNbZ2YvmtkuM7ujjHWsMLNHzOwFM9tmZh8Ph/+pmXWY2TPh7R1lqm+vmT0f1rAlHNZkZg+Z2c7wb+Mc13R+yXJ5xsz6zOwT5VhmZrbBzI6E11gYHTbp8rHAV8Lv3HNmduUc1/WXZrYjfO37zKwhHN5qZsMly+3u2arrJLVN+dmZ2WfCZfaimd04x3X9S0lNe83smXD4nC2zk2TE7H3P3P2X/kZwVs3dwBogBTwLXFimWpYAV4b3awmut3shwUVYPjkPltVeYMGEYX8B3BHevwP4Upk/y1eAVeVYZsAbgSuBradaPsA7gPsBA64Ffj7Hdb0dSIT3v1RSV2vpdGVaZpN+duH/wrNAGlgd/t/G56quCeP/F3DnXC+zk2TErH3PotKivxrY5e573D0L3AvcUo5C3P2Quz8V3u8HtgPLylHLabgF+Ifw/j8A7y5jLW8Fdrt7WS4z5u7/SXCq7VJTLZ9bgH/0wBNAg5ktmau63P1Bdx+9AOoTBBf2mXNTLLOp3ALc6+4Zd38Z2EXw/zundZmZAR8AvjMbr30yJ8mIWfueRSXolwEHSh63Mw/C1cxagSuAn4eDPhZuem2Y6+6REg48aGZPWnAJR4BF7n4ovP8KsKg8pQHBtQxK//nmwzKbavnMp+/d7xC0+katNrOnzewnZnZ9mWqa7LObL8vseuCwu+8sGTbny2xCRsza9ywqQT/vmFkN8H3gE+7eB3wNOAe4HDhEsNlYDm9w9yuBm4A/MLM3lo70YFuxLIdiWXAFs5uBfw0HzZdlNqacy2cqZvZZggv7/HM46BCw0t2vAP4I+LaZ1c1xWfPus5vgQ4xvUMz5MpskI8bM9PcsKkE/rWvTzhUzSxJ8gP/s7v8G4O6H3b3g7kXgG8zS5uqpuHtH+PcIcF9Yx+HRTcHw75Fy1Eaw8nnK3Q+HNc6LZcbUy6fs3zsz+y3gV4FfD8OBsFukK7z/JEE/+HlzWddJPrv5sMwSwHuBfxkdNtfLbLKMYBa/Z1EJ+ulc13ZOhH1/3wS2u/uXS4aX9qm9B9g6cd45qK3azGpH7xPszNvK+Gv+3g78YK5rC41rZc2HZRaaavlsBH4zPCriWqC3ZNN71pnZOuCPgZvdfahkeIuZxcP7awiu1bxnruoKX3eqz24+XEf6bcAOd28fHTCXy2yqjGA2v2dzsZd5Lm4Ee6ZfIlgTf7aMdbyBYJPrOeCZ8PYO4J+A58PhG4ElZahtDcERD88C20aXE9AM/AjYCTwMNJWhtmqgC6gvGTbny4xgRXMIyBH0hX5kquVDcBTEV8Pv3PNA2xzXtYug73b0e3Z3OO37ws/3GeAp4F1lWGZTfnbAZ8Nl9iJw01zWFQ7/e+CjE6ads2V2koyYte+ZfhkrIhJxUem6ERGRKSjoRUQiTkEvIhJxCnoRkYhT0IuIRJyCXkQk4hT0IiIRp6AXEYm4/w/DRNINr6LgbQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-TJhPzCfGPfU",
        "outputId": "0b28a734-5f5b-452c-e222-d7daf0a0ab90"
      },
      "source": [
        "# train autoencoder for classification with with compression in the bottleneck layer\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.layers import BatchNormalization\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from matplotlib import pyplot\r\n",
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# number of input columns\r\n",
        "n_inputs = X.shape[1]\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define encoder\r\n",
        "visible = Input(shape=(n_inputs,))\r\n",
        "# encoder level 1\r\n",
        "e = Dense(n_inputs*2)(visible)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# encoder level 2\r\n",
        "e = Dense(n_inputs)(e)\r\n",
        "e = BatchNormalization()(e)\r\n",
        "e = LeakyReLU()(e)\r\n",
        "# bottleneck\r\n",
        "n_bottleneck = round(float(n_inputs) / 2.0)\r\n",
        "bottleneck = Dense(n_bottleneck)(e)\r\n",
        "# define decoder, level 1\r\n",
        "d = Dense(n_inputs)(bottleneck)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# decoder level 2\r\n",
        "d = Dense(n_inputs*2)(d)\r\n",
        "d = BatchNormalization()(d)\r\n",
        "d = LeakyReLU()(d)\r\n",
        "# output layer\r\n",
        "output = Dense(n_inputs, activation='linear')(d)\r\n",
        "# define autoencoder model\r\n",
        "model = Model(inputs=visible, outputs=output)\r\n",
        "# compile autoencoder model\r\n",
        "model.compile(optimizer='adam', loss='mse')\r\n",
        "# plot the autoencoder\r\n",
        "plot_model(model, 'autoencoder_compress.png', show_shapes=True)\r\n",
        "# fit the autoencoder model to reconstruct input\r\n",
        "history = model.fit(X_train, X_train, epochs=200, batch_size=16, verbose=2, validation_data=(X_test,X_test))\r\n",
        "# plot loss\r\n",
        "pyplot.plot(history.history['loss'], label='train')\r\n",
        "pyplot.plot(history.history['val_loss'], label='test')\r\n",
        "pyplot.legend()\r\n",
        "pyplot.show()\r\n",
        "# define an encoder model (without the decoder)\r\n",
        "encoder = Model(inputs=visible, outputs=bottleneck)\r\n",
        "plot_model(encoder, 'encoder_compress.png', show_shapes=True)\r\n",
        "# save the encoder to file\r\n",
        "encoder.save('encoder.h5')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "42/42 - 2s - loss: 0.2293 - val_loss: 0.1767\n",
            "Epoch 2/200\n",
            "42/42 - 0s - loss: 0.0359 - val_loss: 0.1055\n",
            "Epoch 3/200\n",
            "42/42 - 0s - loss: 0.0223 - val_loss: 0.0558\n",
            "Epoch 4/200\n",
            "42/42 - 0s - loss: 0.0190 - val_loss: 0.0318\n",
            "Epoch 5/200\n",
            "42/42 - 0s - loss: 0.0162 - val_loss: 0.0196\n",
            "Epoch 6/200\n",
            "42/42 - 0s - loss: 0.0147 - val_loss: 0.0139\n",
            "Epoch 7/200\n",
            "42/42 - 0s - loss: 0.0134 - val_loss: 0.0107\n",
            "Epoch 8/200\n",
            "42/42 - 0s - loss: 0.0120 - val_loss: 0.0094\n",
            "Epoch 9/200\n",
            "42/42 - 0s - loss: 0.0118 - val_loss: 0.0081\n",
            "Epoch 10/200\n",
            "42/42 - 0s - loss: 0.0113 - val_loss: 0.0067\n",
            "Epoch 11/200\n",
            "42/42 - 0s - loss: 0.0101 - val_loss: 0.0064\n",
            "Epoch 12/200\n",
            "42/42 - 0s - loss: 0.0093 - val_loss: 0.0058\n",
            "Epoch 13/200\n",
            "42/42 - 0s - loss: 0.0094 - val_loss: 0.0062\n",
            "Epoch 14/200\n",
            "42/42 - 0s - loss: 0.0088 - val_loss: 0.0064\n",
            "Epoch 15/200\n",
            "42/42 - 0s - loss: 0.0086 - val_loss: 0.0062\n",
            "Epoch 16/200\n",
            "42/42 - 0s - loss: 0.0085 - val_loss: 0.0053\n",
            "Epoch 17/200\n",
            "42/42 - 0s - loss: 0.0087 - val_loss: 0.0050\n",
            "Epoch 18/200\n",
            "42/42 - 0s - loss: 0.0086 - val_loss: 0.0051\n",
            "Epoch 19/200\n",
            "42/42 - 0s - loss: 0.0084 - val_loss: 0.0050\n",
            "Epoch 20/200\n",
            "42/42 - 0s - loss: 0.0079 - val_loss: 0.0047\n",
            "Epoch 21/200\n",
            "42/42 - 0s - loss: 0.0074 - val_loss: 0.0047\n",
            "Epoch 22/200\n",
            "42/42 - 0s - loss: 0.0072 - val_loss: 0.0039\n",
            "Epoch 23/200\n",
            "42/42 - 0s - loss: 0.0071 - val_loss: 0.0034\n",
            "Epoch 24/200\n",
            "42/42 - 0s - loss: 0.0067 - val_loss: 0.0035\n",
            "Epoch 25/200\n",
            "42/42 - 0s - loss: 0.0067 - val_loss: 0.0053\n",
            "Epoch 26/200\n",
            "42/42 - 0s - loss: 0.0068 - val_loss: 0.0049\n",
            "Epoch 27/200\n",
            "42/42 - 0s - loss: 0.0074 - val_loss: 0.0049\n",
            "Epoch 28/200\n",
            "42/42 - 0s - loss: 0.0062 - val_loss: 0.0046\n",
            "Epoch 29/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0033\n",
            "Epoch 30/200\n",
            "42/42 - 0s - loss: 0.0060 - val_loss: 0.0036\n",
            "Epoch 31/200\n",
            "42/42 - 0s - loss: 0.0062 - val_loss: 0.0041\n",
            "Epoch 32/200\n",
            "42/42 - 0s - loss: 0.0060 - val_loss: 0.0030\n",
            "Epoch 33/200\n",
            "42/42 - 0s - loss: 0.0062 - val_loss: 0.0036\n",
            "Epoch 34/200\n",
            "42/42 - 0s - loss: 0.0064 - val_loss: 0.0051\n",
            "Epoch 35/200\n",
            "42/42 - 0s - loss: 0.0061 - val_loss: 0.0030\n",
            "Epoch 36/200\n",
            "42/42 - 0s - loss: 0.0059 - val_loss: 0.0042\n",
            "Epoch 37/200\n",
            "42/42 - 0s - loss: 0.0063 - val_loss: 0.0037\n",
            "Epoch 38/200\n",
            "42/42 - 0s - loss: 0.0057 - val_loss: 0.0031\n",
            "Epoch 39/200\n",
            "42/42 - 0s - loss: 0.0062 - val_loss: 0.0035\n",
            "Epoch 40/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0027\n",
            "Epoch 41/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0034\n",
            "Epoch 42/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0028\n",
            "Epoch 43/200\n",
            "42/42 - 0s - loss: 0.0057 - val_loss: 0.0038\n",
            "Epoch 44/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0025\n",
            "Epoch 45/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0028\n",
            "Epoch 46/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0026\n",
            "Epoch 47/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0027\n",
            "Epoch 48/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0035\n",
            "Epoch 49/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0028\n",
            "Epoch 50/200\n",
            "42/42 - 0s - loss: 0.0055 - val_loss: 0.0039\n",
            "Epoch 51/200\n",
            "42/42 - 0s - loss: 0.0052 - val_loss: 0.0027\n",
            "Epoch 52/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0020\n",
            "Epoch 53/200\n",
            "42/42 - 0s - loss: 0.0049 - val_loss: 0.0028\n",
            "Epoch 54/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0028\n",
            "Epoch 55/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0022\n",
            "Epoch 56/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0018\n",
            "Epoch 57/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0026\n",
            "Epoch 58/200\n",
            "42/42 - 0s - loss: 0.0053 - val_loss: 0.0024\n",
            "Epoch 59/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0029\n",
            "Epoch 60/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0032\n",
            "Epoch 61/200\n",
            "42/42 - 0s - loss: 0.0051 - val_loss: 0.0024\n",
            "Epoch 62/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0032\n",
            "Epoch 63/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0020\n",
            "Epoch 64/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0022\n",
            "Epoch 65/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0017\n",
            "Epoch 66/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0023\n",
            "Epoch 67/200\n",
            "42/42 - 0s - loss: 0.0049 - val_loss: 0.0029\n",
            "Epoch 68/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0027\n",
            "Epoch 69/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0019\n",
            "Epoch 70/200\n",
            "42/42 - 0s - loss: 0.0049 - val_loss: 0.0031\n",
            "Epoch 71/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0020\n",
            "Epoch 72/200\n",
            "42/42 - 0s - loss: 0.0048 - val_loss: 0.0024\n",
            "Epoch 73/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0022\n",
            "Epoch 74/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0018\n",
            "Epoch 75/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0033\n",
            "Epoch 76/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0019\n",
            "Epoch 77/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0029\n",
            "Epoch 78/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0020\n",
            "Epoch 79/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0020\n",
            "Epoch 80/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 81/200\n",
            "42/42 - 0s - loss: 0.0049 - val_loss: 0.0018\n",
            "Epoch 82/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0025\n",
            "Epoch 83/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0015\n",
            "Epoch 84/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0017\n",
            "Epoch 85/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0017\n",
            "Epoch 86/200\n",
            "42/42 - 0s - loss: 0.0050 - val_loss: 0.0026\n",
            "Epoch 87/200\n",
            "42/42 - 0s - loss: 0.0045 - val_loss: 0.0023\n",
            "Epoch 88/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0018\n",
            "Epoch 89/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 90/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0019\n",
            "Epoch 91/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0024\n",
            "Epoch 92/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 93/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0019\n",
            "Epoch 94/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0025\n",
            "Epoch 95/200\n",
            "42/42 - 0s - loss: 0.0047 - val_loss: 0.0026\n",
            "Epoch 96/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0027\n",
            "Epoch 97/200\n",
            "42/42 - 0s - loss: 0.0044 - val_loss: 0.0026\n",
            "Epoch 98/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0017\n",
            "Epoch 99/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0019\n",
            "Epoch 100/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0016\n",
            "Epoch 101/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0026\n",
            "Epoch 102/200\n",
            "42/42 - 0s - loss: 0.0042 - val_loss: 0.0026\n",
            "Epoch 103/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0016\n",
            "Epoch 104/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0020\n",
            "Epoch 105/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0030\n",
            "Epoch 106/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0021\n",
            "Epoch 107/200\n",
            "42/42 - 0s - loss: 0.0046 - val_loss: 0.0028\n",
            "Epoch 108/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 109/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 110/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0017\n",
            "Epoch 111/200\n",
            "42/42 - 0s - loss: 0.0043 - val_loss: 0.0029\n",
            "Epoch 112/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0019\n",
            "Epoch 113/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0012\n",
            "Epoch 114/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0019\n",
            "Epoch 115/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0022\n",
            "Epoch 116/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0024\n",
            "Epoch 117/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 118/200\n",
            "42/42 - 0s - loss: 0.0041 - val_loss: 0.0018\n",
            "Epoch 119/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0023\n",
            "Epoch 120/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0017\n",
            "Epoch 121/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0019\n",
            "Epoch 122/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 123/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0023\n",
            "Epoch 124/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0017\n",
            "Epoch 125/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0026\n",
            "Epoch 126/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0018\n",
            "Epoch 127/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 128/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0015\n",
            "Epoch 129/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0025\n",
            "Epoch 130/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0021\n",
            "Epoch 131/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0022\n",
            "Epoch 132/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0017\n",
            "Epoch 133/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0021\n",
            "Epoch 134/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0021\n",
            "Epoch 135/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0022\n",
            "Epoch 136/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0023\n",
            "Epoch 137/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 138/200\n",
            "42/42 - 0s - loss: 0.0040 - val_loss: 0.0027\n",
            "Epoch 139/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0014\n",
            "Epoch 140/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0014\n",
            "Epoch 141/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0015\n",
            "Epoch 142/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0012\n",
            "Epoch 143/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0014\n",
            "Epoch 144/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0020\n",
            "Epoch 145/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0012\n",
            "Epoch 146/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0016\n",
            "Epoch 147/200\n",
            "42/42 - 0s - loss: 0.0039 - val_loss: 0.0018\n",
            "Epoch 148/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0022\n",
            "Epoch 149/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0020\n",
            "Epoch 150/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0015\n",
            "Epoch 151/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 152/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0021\n",
            "Epoch 153/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0015\n",
            "Epoch 154/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0017\n",
            "Epoch 155/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0020\n",
            "Epoch 156/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0014\n",
            "Epoch 157/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0013\n",
            "Epoch 158/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
            "Epoch 159/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 160/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0013\n",
            "Epoch 161/200\n",
            "42/42 - 0s - loss: 0.0035 - val_loss: 0.0017\n",
            "Epoch 162/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 163/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0016\n",
            "Epoch 164/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0022\n",
            "Epoch 165/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
            "Epoch 166/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
            "Epoch 167/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0013\n",
            "Epoch 168/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0015\n",
            "Epoch 169/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0018\n",
            "Epoch 170/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0017\n",
            "Epoch 171/200\n",
            "42/42 - 0s - loss: 0.0036 - val_loss: 0.0018\n",
            "Epoch 172/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0013\n",
            "Epoch 173/200\n",
            "42/42 - 0s - loss: 0.0037 - val_loss: 0.0016\n",
            "Epoch 174/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 9.3847e-04\n",
            "Epoch 175/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0013\n",
            "Epoch 176/200\n",
            "42/42 - 0s - loss: 0.0038 - val_loss: 0.0010\n",
            "Epoch 177/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0015\n",
            "Epoch 178/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0018\n",
            "Epoch 179/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0011\n",
            "Epoch 180/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0011\n",
            "Epoch 181/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0010\n",
            "Epoch 182/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0021\n",
            "Epoch 183/200\n",
            "42/42 - 0s - loss: 0.0034 - val_loss: 0.0018\n",
            "Epoch 184/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 8.9389e-04\n",
            "Epoch 185/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0012\n",
            "Epoch 186/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0013\n",
            "Epoch 187/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0016\n",
            "Epoch 188/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 9.4922e-04\n",
            "Epoch 189/200\n",
            "42/42 - 0s - loss: 0.0033 - val_loss: 0.0014\n",
            "Epoch 190/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0014\n",
            "Epoch 191/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 9.4201e-04\n",
            "Epoch 192/200\n",
            "42/42 - 0s - loss: 0.0030 - val_loss: 0.0012\n",
            "Epoch 193/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n",
            "Epoch 194/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0012\n",
            "Epoch 195/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0011\n",
            "Epoch 196/200\n",
            "42/42 - 0s - loss: 0.0027 - val_loss: 8.7613e-04\n",
            "Epoch 197/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0012\n",
            "Epoch 198/200\n",
            "42/42 - 0s - loss: 0.0031 - val_loss: 0.0013\n",
            "Epoch 199/200\n",
            "42/42 - 0s - loss: 0.0029 - val_loss: 0.0014\n",
            "Epoch 200/200\n",
            "42/42 - 0s - loss: 0.0032 - val_loss: 0.0012\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1qXoxgHGn6M",
        "outputId": "53f29aa0-4bda-4ee5-d1fc-41b1afdfbd47"
      },
      "source": [
        "# baseline in performance with logistic regression model\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# define model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit model on training set\r\n",
        "model.fit(X_train, y_train)\r\n",
        "# make prediction on test set\r\n",
        "yhat = model.predict(X_test)\r\n",
        "# calculate accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8939393939393939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKlVIQb7GxVz",
        "outputId": "2de7360c-ea0c-4ef6-afe1-17684053e636"
      },
      "source": [
        "# evaluate logistic regression on encoded input\r\n",
        "from sklearn.datasets import make_classification\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "# define dataset\r\n",
        "X, y = make_classification(n_samples=1000, n_features=100, n_informative=10, n_redundant=90, random_state=1)\r\n",
        "# split into train test sets\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\r\n",
        "# scale data\r\n",
        "t = MinMaxScaler()\r\n",
        "t.fit(X_train)\r\n",
        "X_train = t.transform(X_train)\r\n",
        "X_test = t.transform(X_test)\r\n",
        "# load the model from file\r\n",
        "encoder = load_model('encoder.h5')\r\n",
        "# encode the train data\r\n",
        "X_train_encode = encoder.predict(X_train)\r\n",
        "# encode the test data\r\n",
        "X_test_encode = encoder.predict(X_test)\r\n",
        "# define the model\r\n",
        "model = LogisticRegression()\r\n",
        "# fit the model on the training set\r\n",
        "model.fit(X_train_encode, y_train)\r\n",
        "# make predictions on the test set\r\n",
        "yhat = model.predict(X_test_encode)\r\n",
        "# calculate classification accuracy\r\n",
        "acc = accuracy_score(y_test, yhat)\r\n",
        "print(acc)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
            "0.9090909090909091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}